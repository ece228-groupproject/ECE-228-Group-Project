{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb45a9b5",
   "metadata": {},
   "source": [
    "# Training and Testing A Visual Transformer Model\n",
    "Here we test a visual Transfrom ViT from [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.v2 as v2\n",
    "from our_datasets import Country_images\n",
    "from Country_dict import country_dict\n",
    "from ViT import VisionTransformer\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#get models\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14315f",
   "metadata": {},
   "source": [
    "## Load our Dataset and Create our dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e496297",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "weights = tv.models.ViT_L_32_Weights.DEFAULT\n",
    "transform = v2.Compose([weights.transforms(), ])\n",
    "\n",
    "num_classes = len(country_dict)\n",
    "dataset_path = \"data\\\\compressed_dataset\\\\\"\n",
    "dataset = Country_images(\"data\\\\compressed_dataset\\\\country.csv\",dataset_path,transform=transform)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,lengths=[0.7,0.1,0.2])\n",
    "data_loader_params = {\n",
    "    'batch_size': batch_size,  # Batch size for data loading\n",
    "    'num_workers': 10,  # Number of subprocesses to use for data loading\n",
    "    'persistent_workers': True,  # If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.\n",
    "    'pin_memory': True,  # If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.\n",
    "    'pin_memory_device': 'cuda' ,  # Specifies the device where the data should be loaded. Commonly set to use the GPU.\n",
    "}\n",
    "train_dataloader      = torch.utils.data.DataLoader(train_dataset, **data_loader_params, shuffle=True)\n",
    "val_dataloader        = torch.utils.data.DataLoader(val_dataset, **data_loader_params, shuffle=True)\n",
    "test_dataloader       = torch.utils.data.DataLoader(test_dataset, **data_loader_params, shuffle=False,in_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79833d2",
   "metadata": {},
   "source": [
    "## Load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc4dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained_Models\\ViT\\\n",
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 1024, kernel_size=(32, 32), stride=(32, 32))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_12): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_13): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_14): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_15): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_16): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_17): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_18): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_19): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_20): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_21): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_22): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_23): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=1024, out_features=124, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters:\n",
    "lr = 0.1\n",
    "#maximum learning rate we will let our model train in order to train faster at the start\n",
    "max_lr = 0.1\n",
    "weight_decay = 0.0\n",
    "EPOCHS = 100\n",
    "#end hyperparameters\n",
    "\n",
    "#model and optimizers\n",
    "model = tv.models.vit_l_32(weights=weights)\n",
    "#model = VisionTransformer(num_classes=num_classes)\n",
    "model.device = device\n",
    "model.name = \"ViT_l_16\"\n",
    "model.path = \"Trained_Models\\\\ViT\\\\\" #where to save our best model\n",
    "print(model.path)\n",
    "#redfine our output layer to output our classes\n",
    "model.heads.head = torch.nn.Linear(in_features=model.heads.head.in_features,out_features=num_classes,bias=True)\n",
    "print(model)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#scales the gradients, neccessary for mixed precision data types to properly converge\n",
    "scaler = torch.amp.GradScaler(device=device)\n",
    "#change our learning rate based on far we are in training and if we are improving\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=EPOCHS*len(train_dataloader))\n",
    "\n",
    "#added data to our model for ease of use (and to prevent passing so many variables to our training function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304dc7e",
   "metadata": {},
   "source": [
    "## Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 4.360601425170898\n",
      "  batch 200 loss: 4.322263240814209\n",
      "  batch 300 loss: 4.3097076416015625\n",
      "  batch 400 loss: 4.305140495300293\n",
      "  batch 500 loss: 4.301965236663818\n",
      "  batch 600 loss: 4.299342632293701\n",
      "  batch 700 loss: 4.29555082321167\n",
      "  batch 800 loss: 4.293180465698242\n",
      "  batch 900 loss: 4.291337013244629\n",
      "  batch 1000 loss: 4.290902614593506\n",
      "LOSS train 4.290902614593506 valid 19.809770584106445\n",
      "Accuracy: 0.0188\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 4.323410511016846\n",
      "  batch 200 loss: 4.295217037200928\n",
      "  batch 300 loss: 4.291136264801025\n",
      "  batch 400 loss: 4.286738395690918\n",
      "  batch 500 loss: 4.2853569984436035\n",
      "  batch 600 loss: 4.2825093269348145\n",
      "  batch 700 loss: 4.283159255981445\n",
      "  batch 800 loss: 4.283255100250244\n",
      "  batch 900 loss: 4.282806873321533\n",
      "  batch 1000 loss: 4.28232479095459\n",
      "LOSS train 4.28232479095459 valid 25.436050415039062\n",
      "Accuracy: 0.013\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 4.3167266845703125\n",
      "  batch 200 loss: 4.3024115562438965\n",
      "  batch 300 loss: 4.2920403480529785\n",
      "  batch 400 loss: 4.287500381469727\n",
      "  batch 500 loss: 4.286408424377441\n",
      "  batch 600 loss: 4.28484582901001\n",
      "  batch 700 loss: 4.282524108886719\n",
      "  batch 800 loss: 4.2819952964782715\n",
      "  batch 900 loss: 4.281827449798584\n",
      "  batch 1000 loss: 4.281974792480469\n",
      "LOSS train 4.281974792480469 valid 25.376462936401367\n",
      "Accuracy: 0.013\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 4.3220930099487305\n",
      "  batch 200 loss: 4.299584865570068\n",
      "  batch 300 loss: 4.290338516235352\n",
      "  batch 400 loss: 4.288388729095459\n",
      "  batch 500 loss: 4.28765869140625\n",
      "  batch 600 loss: 4.284929275512695\n",
      "  batch 700 loss: 4.285305500030518\n",
      "  batch 800 loss: 4.285821914672852\n",
      "  batch 900 loss: 4.284589767456055\n",
      "  batch 1000 loss: 4.283792495727539\n",
      "LOSS train 4.283792495727539 valid 24.190250396728516\n",
      "Accuracy: 0.013\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 4.327404499053955\n",
      "  batch 200 loss: 4.304554462432861\n",
      "  batch 300 loss: 4.294376373291016\n",
      "  batch 400 loss: 4.292275905609131\n",
      "  batch 500 loss: 4.2901411056518555\n",
      "  batch 600 loss: 4.285953521728516\n",
      "  batch 700 loss: 4.285423278808594\n",
      "  batch 800 loss: 4.285416603088379\n",
      "  batch 900 loss: 4.2847161293029785\n",
      "  batch 1000 loss: 4.283280372619629\n",
      "LOSS train 4.283280372619629 valid 24.286605834960938\n",
      "Accuracy: 0.013\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 4.330245018005371\n",
      "  batch 200 loss: 4.303612232208252\n",
      "  batch 300 loss: 4.292285919189453\n",
      "  batch 400 loss: 4.287185192108154\n",
      "  batch 500 loss: 4.287322521209717\n",
      "  batch 600 loss: 4.286787986755371\n",
      "  batch 700 loss: 4.286406517028809\n",
      "  batch 800 loss: 4.285416603088379\n",
      "  batch 900 loss: 4.284820556640625\n",
      "  batch 1000 loss: 4.284312725067139\n",
      "LOSS train 4.284312725067139 valid 24.0545654296875\n",
      "Accuracy: 0.013\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 4.321406841278076\n",
      "  batch 200 loss: 4.300942897796631\n",
      "  batch 300 loss: 4.299393177032471\n",
      "  batch 400 loss: 4.295957088470459\n",
      "  batch 500 loss: 4.295025825500488\n",
      "  batch 600 loss: 4.291483402252197\n",
      "  batch 700 loss: 4.287971496582031\n",
      "  batch 800 loss: 4.286081790924072\n",
      "  batch 900 loss: 4.284403324127197\n",
      "  batch 1000 loss: 4.284218788146973\n",
      "LOSS train 4.284218788146973 valid 24.151992797851562\n",
      "Accuracy: 0.013\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 4.323300361633301\n",
      "  batch 200 loss: 4.301257133483887\n",
      "  batch 300 loss: 4.289673328399658\n",
      "  batch 400 loss: 4.288516521453857\n",
      "  batch 500 loss: 4.288575172424316\n",
      "  batch 600 loss: 4.288196563720703\n",
      "  batch 700 loss: 4.286540985107422\n",
      "  batch 800 loss: 4.285064697265625\n",
      "  batch 900 loss: 4.284611701965332\n",
      "  batch 1000 loss: 4.283812046051025\n",
      "LOSS train 4.283812046051025 valid 24.292051315307617\n",
      "Accuracy: 0.013\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 4.334348201751709\n",
      "  batch 200 loss: 4.307224273681641\n",
      "  batch 300 loss: 4.29887056350708\n",
      "  batch 400 loss: 4.295252323150635\n",
      "  batch 500 loss: 4.291894435882568\n",
      "  batch 600 loss: 4.288561820983887\n",
      "  batch 700 loss: 4.286853790283203\n",
      "  batch 800 loss: 4.286042213439941\n",
      "  batch 900 loss: 4.285411357879639\n",
      "  batch 1000 loss: 4.2847819328308105\n",
      "LOSS train 4.2847819328308105 valid 24.393959045410156\n",
      "Accuracy: 0.013\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 4.333717346191406\n",
      "  batch 200 loss: 4.306753158569336\n",
      "  batch 300 loss: 4.296989440917969\n",
      "  batch 400 loss: 4.290474891662598\n",
      "  batch 500 loss: 4.287134647369385\n",
      "  batch 600 loss: 4.287257671356201\n",
      "  batch 700 loss: 4.287524223327637\n",
      "  batch 800 loss: 4.285846710205078\n",
      "  batch 900 loss: 4.284820556640625\n",
      "  batch 1000 loss: 4.284062385559082\n",
      "LOSS train 4.284062385559082 valid 24.482580184936523\n",
      "Accuracy: 0.013\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 4.31730318069458\n",
      "  batch 200 loss: 4.298744201660156\n",
      "  batch 300 loss: 4.292285919189453\n",
      "  batch 400 loss: 4.289299964904785\n",
      "  batch 500 loss: 4.2856316566467285\n",
      "  batch 600 loss: 4.284701347351074\n",
      "  batch 700 loss: 4.2862725257873535\n",
      "  batch 800 loss: 4.285064697265625\n",
      "  batch 900 loss: 4.283777713775635\n",
      "  batch 1000 loss: 4.2833428382873535\n",
      "LOSS train 4.2833428382873535 valid 99.75797271728516\n",
      "Accuracy: 0.013\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 4.332317352294922\n",
      "  batch 200 loss: 4.317150115966797\n",
      "  batch 300 loss: 4.307101249694824\n",
      "  batch 400 loss: 4.301456928253174\n",
      "  batch 500 loss: 4.3007049560546875\n",
      "  batch 600 loss: 4.297595500946045\n",
      "  batch 700 loss: 4.296806812286377\n",
      "  batch 800 loss: 4.296293258666992\n",
      "  batch 900 loss: 4.296067714691162\n",
      "  batch 1000 loss: 4.293677806854248\n",
      "LOSS train 4.293677806854248 valid 100.95150756835938\n",
      "Accuracy: 0.013\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 4.338630676269531\n",
      "  batch 200 loss: 4.3163652420043945\n",
      "  batch 300 loss: 4.306264877319336\n",
      "  batch 400 loss: 4.301143646240234\n",
      "  batch 500 loss: 4.300266742706299\n",
      "  batch 600 loss: 4.297595500946045\n",
      "  batch 700 loss: 4.2973432540893555\n",
      "  batch 800 loss: 4.295706748962402\n",
      "  batch 900 loss: 4.295198917388916\n",
      "  batch 1000 loss: 4.2938032150268555\n",
      "LOSS train 4.2938032150268555 valid 100.9929428100586\n",
      "Accuracy: 0.013\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 4.3313703536987305\n",
      "  batch 200 loss: 4.3118109703063965\n",
      "  batch 300 loss: 4.307205677032471\n",
      "  batch 400 loss: 4.299263954162598\n",
      "  batch 500 loss: 4.2980122566223145\n",
      "  batch 600 loss: 4.297543525695801\n",
      "  batch 700 loss: 4.295823097229004\n",
      "  batch 800 loss: 4.294846057891846\n",
      "  batch 900 loss: 4.294955253601074\n",
      "  batch 1000 loss: 4.294428825378418\n",
      "LOSS train 4.294428825378418 valid 100.95653533935547\n",
      "Accuracy: 0.013\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 4.335789680480957\n",
      "  batch 200 loss: 4.30961275100708\n",
      "  batch 300 loss: 4.305219650268555\n",
      "  batch 400 loss: 4.299733638763428\n",
      "  batch 500 loss: 4.297323226928711\n",
      "  batch 600 loss: 4.296291351318359\n",
      "  batch 700 loss: 4.295644283294678\n",
      "  batch 800 loss: 4.2941813468933105\n",
      "  batch 900 loss: 4.2945380210876465\n",
      "  batch 1000 loss: 4.294991493225098\n",
      "LOSS train 4.294991493225098 valid 100.94402313232422\n",
      "Accuracy: 0.013\n",
      "EPOCH 16:\n"
     ]
    }
   ],
   "source": [
    "#we will call the function we defined in \"Training_Functions.py\"\n",
    "from Training_Functions import TrainModel\n",
    "model_train = True\n",
    "\n",
    "if model_train:\n",
    "    model = model.to(device)\n",
    "    TrainModel(model,EPOCHS, loss_fn, train_dataloader, val_dataloader, optimizer, lr_scheduler, scaler)\n",
    "if not model_train:\n",
    "    checkpoint = torch.load(model.path+model.name+\"-best\")\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46ee3d",
   "metadata": {},
   "source": [
    "## Test our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b243328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will call the function we defined in \"Training_Functions.py\"\n",
    "from Training_Functions import TestModel\n",
    "TestModel(model, test_dataloader, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
